# API Code Review

The API development has chosen to utilize an asynchronous communication protocol
between the API and its associated database, facilitating the receipt of relevant
data.

The application has been broken down into multiple files, each with its own role and
responsible for performing a specific task. This approach allows us to achieve
modularity while maintaining a simple and easy-to-read solution.

The scalability of the system could be enhanced by integrating a message queueing
mechanism, such as RabbitMQ, to handle asynchronous communication between components,
while upgrading to a more robust and scalable database solution.

## Features

* The process is completely modular and reusable.
* Asynchronous file processing for increased multitasking
* Easy-to-maintain solution
* Integration with database for communication between different endpoints

## Tech

The implementation of this API has been carried out using the following libraries:

* **FastAPI**: Framework used for creating the API itself
* **Uvicorn**: The ASGI server that we will use in this project
* **SQLAlchemy**: The library we will use to implement database reading

The development environment consisted of Ubuntu 24.04 and Python 3.12 with a virtualenv. As such, the solution is expected to be backwards compatible with earlier versions of Python, including those from Python 2.x to Python 3.x.

## Code Analysis for API

In this README, a general analysis will be made of how each file works and their
corresponding functions. However, for more detailed information about each method, it is
recommended to review the code itself and consult the docstrings associated with each
function.

### 1. `main.py`

This file contains the main execution of the API, and it's here where you'll find the
following functions:

* **`get_session`**: Helper method that allows us to getting the session for the database and creates a generator __(yield)__ that takes care of handling and providing data from the database stored in memory.
  * __Time Complexity__: O(1)

* **`save_upload_file`**: Helper method that allows us to serves to save the uploaded file to the disk and work with it.
  * __Time Complexity__: O(n) where n is the size of the file.

* **`upload_csv`**: This method creates an asynchronous POST request, allowing us to upload files that can be processed later with the CSV processor. It also creates a database record for the process.
  * __Time Complexity__: O(n) where n is the size of the file.

* **`get_task_status`**: This method is in charge of creating a GET endpoint to obtain the results from the
previous endpoint. It queries the database and returns the processing data.
  * __Time Complexity__: O(1)


### 2. `tasks/tasks.py`

On this file we will have the tasks that will be carried out by different tasks to then
be called through endpoints.

* **`process_csv_task`**: It will be responsible for executing the process to process CSV files from the other module developed for this task. This method includes a debug mode to simulate the processing of large files in case they take time to process.
  * __Time Complexity__: O(n) where n is the number of rows in the CSV file

### 3. `processors/csv_processor.py`

It is further explained in detail in the README for the other task.

### 4. `database/database.py`

On this file, we will take care of creating a connection to a database and then
creating its respective table with its schema

* __Time Complexity__: O(t * c) where t is the number of tables and c is the average number of columns per table.

### 5. `database/models.py`

This file will have the database schemas generated by each task

* **`Task`**: This class will be responsible for initializing the table and columns that will be incorporated by each registered task.
  * __Time Complexity__: O(1)


</br>

# [`Go Back`](../README.md)
